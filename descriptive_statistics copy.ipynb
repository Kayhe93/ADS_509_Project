{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eJEnbivzdOdl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from pandas import json_normalize\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import pprint\n",
        "from string import punctuation\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words(\"english\")\n",
        "sw.append('recipes')\n",
        "sw.append('recipe')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WQrHvfa7dOdm"
      },
      "outputs": [],
      "source": [
        "# define functions\n",
        "\n",
        "punctuation = set(punctuation) # speeds up comparison\n",
        "tw_punct = punctuation\n",
        "\n",
        "\n",
        "def descriptive_stats(tokens, verbose=True) :\n",
        "    \"\"\"\n",
        "        Given a list of tokens, print number of tokens, number of unique tokens,\n",
        "        number of characters, lexical diversity, and num_tokens most common\n",
        "        tokens. Return a list of\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    num_tokens=len(tokens)\n",
        "    num_unique_tokens = len(set(tokens))\n",
        "    lexical_diversity = num_unique_tokens/num_tokens\n",
        "    num_characters = sum(len(token) for token in tokens)\n",
        "\n",
        "    if verbose :\n",
        "        print(f\"There are {num_tokens} tokens in the data.\")\n",
        "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
        "        print(f\"There are {num_characters} characters in the data.\")\n",
        "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
        "\n",
        "        # print the five most common tokens\n",
        "        counter = Counter(tokens)\n",
        "        top_5_tokens = counter.most_common(5)\n",
        "        print(\"Top 5 most common tokens:\")\n",
        "        for token, count in top_5_tokens:\n",
        "            print(f\"{token}: {count} occurrences\")\n",
        "\n",
        "    return([num_tokens, num_unique_tokens,\n",
        "            lexical_diversity,\n",
        "            num_characters])\n",
        "\n",
        "\n",
        "def remove_stopwords(tokens) :\n",
        "    return [token for token in tokens if token not in sw]\n",
        "    return(tokens)\n",
        "\n",
        "def remove_punctuation(text, punct_set=tw_punct) :\n",
        "    \"\"\"\n",
        "        Function takes two arguments: (1) text, which is the input string, and (2) the punctuation set, which is set to the tw_punct value set.\n",
        "        Returns all characters not found in the punctuation set and concatenates them back into a string using the .join() method with an empty\n",
        "        string \"\" as the separator.\n",
        "    \"\"\"\n",
        "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
        "\n",
        "def tokenize(text) :\n",
        "    \"\"\"\n",
        "        Splitting on whitespace rather than the book's tokenize function. That\n",
        "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter.\n",
        "    \"\"\"\n",
        "    tokens = text.split()\n",
        "    return(tokens)\n",
        "\n",
        "def prepare(text, pipeline) :\n",
        "    tokens = str(text)\n",
        "\n",
        "    for transform in pipeline :\n",
        "        tokens = transform(tokens)\n",
        "\n",
        "    return(tokens)\n",
        "def display_topics(model, features, no_top_words=5):\n",
        "    for topic, words in enumerate(model.components_):\n",
        "        total = words.sum()\n",
        "        largest = words.argsort()[::-1] # invert sort order\n",
        "        print(\"\\nTopic %02d\" % topic)\n",
        "        for i in range(0, no_top_words):\n",
        "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EzciidofdOdm"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def wordcloud(word_freq, title, max_words=200, stopwords=sw):\n",
        "\n",
        "    wc = WordCloud(width=800, height=400,\n",
        "                   background_color= \"black\", colormap=\"Paired\",\n",
        "                   max_font_size=150, max_words=max_words)\n",
        "\n",
        "    # convert data frame into dict\n",
        "    if type(word_freq) == pd.Series:\n",
        "        counter = Counter(word_freq.fillna(0).to_dict())\n",
        "    else:\n",
        "        counter = word_freq\n",
        "\n",
        "    # filter stop words in frequency counter\n",
        "    if stopwords is not None:\n",
        "        counter = {token:freq for (token, freq) in counter.items()\n",
        "                              if token not in stopwords}\n",
        "    wc.generate_from_frequencies(counter)\n",
        "\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "def count_words(df, column='titles_tokens', preprocess=None, min_freq=2):\n",
        "\n",
        "    # process tokens and update counter\n",
        "    def update(doc):\n",
        "        tokens = doc if preprocess is None else preprocess(doc)\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # create counter and run through all data\n",
        "    counter = Counter()\n",
        "    df[column].map(update)\n",
        "\n",
        "    # transform counter into data frame\n",
        "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
        "    freq_df = freq_df.query('freq >= @min_freq')\n",
        "    freq_df.index.name = 'token'\n",
        "\n",
        "    return freq_df.sort_values('freq', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iVqMk6-XdOdm"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>ingredients</th>\n",
              "      <th>step</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cannellini Bean and Asparagus Salad with Mushr...</td>\n",
              "      <td>cannellini beans, water, water, curry leaves, ...</td>\n",
              "      <td>Rinse the cannellini beans and soak for 8 hour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Berry Banana Breakfast Smoothie</td>\n",
              "      <td>yogurt, graham cracker crumbs, soymilk, berrie...</td>\n",
              "      <td>Take some yogurt in your favorite flavor and a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Red Lentil Soup with Chicken and Turnips</td>\n",
              "      <td>olive oil, soup, carrot, celery, onion, salt a...</td>\n",
              "      <td>To a large dutch oven or soup pot, heat the ol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Asparagus and Pea Soup: Real Convenience Food</td>\n",
              "      <td>garlic, onion, garlic, onion, extra virgin oli...</td>\n",
              "      <td>Chop the garlic and onions. Saute the onions i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Garlicky Kale</td>\n",
              "      <td>olive oil, kale, vinegar, garlic</td>\n",
              "      <td>Heat the olive oil in a large pot over medium ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  Cannellini Bean and Asparagus Salad with Mushr...   \n",
              "1                    Berry Banana Breakfast Smoothie   \n",
              "2           Red Lentil Soup with Chicken and Turnips   \n",
              "3      Asparagus and Pea Soup: Real Convenience Food   \n",
              "4                                      Garlicky Kale   \n",
              "\n",
              "                                         ingredients  \\\n",
              "0  cannellini beans, water, water, curry leaves, ...   \n",
              "1  yogurt, graham cracker crumbs, soymilk, berrie...   \n",
              "2  olive oil, soup, carrot, celery, onion, salt a...   \n",
              "3  garlic, onion, garlic, onion, extra virgin oli...   \n",
              "4                   olive oil, kale, vinegar, garlic   \n",
              "\n",
              "                                                step  \n",
              "0  Rinse the cannellini beans and soak for 8 hour...  \n",
              "1  Take some yogurt in your favorite flavor and a...  \n",
              "2  To a large dutch oven or soup pot, heat the ol...  \n",
              "3  Chop the garlic and onions. Saute the onions i...  \n",
              "4  Heat the olive oil in a large pot over medium ...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# data import\n",
        "recipes = pd.read_csv(\"recipes.csv\")\n",
        "recipes = pd.DataFrame(recipes)\n",
        "#recipes['title']=recipes['title'].astype(str)\n",
        "#recipes['ingredients']=recipes['ingredients'].astype(str)\n",
        "#recipes['step']=recipes['step'].astype(str)\n",
        "#recipes.dtypes\n",
        "\n",
        "recipes.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ftfW37ULdOdn",
        "outputId": "b706512c-0ab5-4c3a-fcd2-a7d7fa96691a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Recipe_tokens</th>\n",
              "      <th>Ingredients_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[cannellini, bean, asparagus, salad, mushrooms]</td>\n",
              "      <td>[cannellini, beans, water, water, curry, leave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[berry, banana, breakfast, smoothie]</td>\n",
              "      <td>[yogurt, graham, cracker, crumbs, soymilk, ber...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[red, lentil, soup, chicken, turnips]</td>\n",
              "      <td>[olive, oil, soup, carrot, celery, onion, salt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[asparagus, pea, soup, real, convenience, food]</td>\n",
              "      <td>[garlic, onion, garlic, onion, extra, virgin, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[garlicky, kale]</td>\n",
              "      <td>[olive, oil, kale, vinegar, garlic]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[slow, cooker, beef, stew]</td>\n",
              "      <td>[cream, mushroom, soup, beef, broth, seasoning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[red, kidney, bean, jambalaya]</td>\n",
              "      <td>[kidney, beans, brown, rice, water, kidney, be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[chicken, fajita, stuffed, bell, pepper]</td>\n",
              "      <td>[chili, powder, cilantro, pepper, quinoa, cumi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[cauliflower, brown, rice, vegetable, fried, r...</td>\n",
              "      <td>[cauliflower, florets, cauliflower, rice, caul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[hummus, zaatar]</td>\n",
              "      <td>[chickpeas, water, water, canned, chickpeas, p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Recipe_tokens  \\\n",
              "0    [cannellini, bean, asparagus, salad, mushrooms]   \n",
              "1               [berry, banana, breakfast, smoothie]   \n",
              "2              [red, lentil, soup, chicken, turnips]   \n",
              "3    [asparagus, pea, soup, real, convenience, food]   \n",
              "4                                   [garlicky, kale]   \n",
              "5                         [slow, cooker, beef, stew]   \n",
              "6                     [red, kidney, bean, jambalaya]   \n",
              "7           [chicken, fajita, stuffed, bell, pepper]   \n",
              "8  [cauliflower, brown, rice, vegetable, fried, r...   \n",
              "9                                   [hummus, zaatar]   \n",
              "\n",
              "                                  Ingredients_tokens  \n",
              "0  [cannellini, beans, water, water, curry, leave...  \n",
              "1  [yogurt, graham, cracker, crumbs, soymilk, ber...  \n",
              "2  [olive, oil, soup, carrot, celery, onion, salt...  \n",
              "3  [garlic, onion, garlic, onion, extra, virgin, ...  \n",
              "4                [olive, oil, kale, vinegar, garlic]  \n",
              "5  [cream, mushroom, soup, beef, broth, seasoning...  \n",
              "6  [kidney, beans, brown, rice, water, kidney, be...  \n",
              "7  [chili, powder, cilantro, pepper, quinoa, cumi...  \n",
              "8  [cauliflower, florets, cauliflower, rice, caul...  \n",
              "9  [chickpeas, water, water, canned, chickpeas, p...  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# store tokens in new dataframe 'df'\n",
        "df=pd.DataFrame()\n",
        "\n",
        "# fold to lowercase\n",
        "df['Recipe_tokens']=recipes['title'].str.lower()\n",
        "df['Ingredients_tokens']=recipes['ingredients'].str.lower()\n",
        "#df['URL_tokens']=recipes['URL'].str.lower()\n",
        "\n",
        "# remove punctuation\n",
        "df['Ingredients_tokens']=df['Ingredients_tokens'].apply(remove_punctuation)\n",
        "#df['URL_tokens']=df['URL_tokens'].apply(remove_punctuation)\n",
        "df['Recipe_tokens']=df['Recipe_tokens'].apply(remove_punctuation)\n",
        "\n",
        "# tokenize\n",
        "df['Recipe_tokens']=tokenize(df['Recipe_tokens'].str)\n",
        "df['Ingredients_tokens']=tokenize(df['Ingredients_tokens'].str)\n",
        "#df['URL_tokens']=tokenize(df['URL_tokens'].str)\n",
        "\n",
        "# remove stopwords\n",
        "df['Ingredients_tokens']=df['Ingredients_tokens'].apply(remove_stopwords)\n",
        "#df['URL_tokens']=df['URL_tokens'].apply(remove_stopwords)\n",
        "df['Recipe_tokens']=df['Recipe_tokens'].apply(remove_stopwords)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXCOYwe-dOdn",
        "outputId": "9e09b1ae-26e0-48eb-9d24-09c5a5dee019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 43 tokens in the data.\n",
            "There are 37 unique tokens in the data.\n",
            "There are 259 characters in the data.\n",
            "The lexical diversity is 0.860 in the data.\n",
            "Top 5 most common tokens:\n",
            "bean: 2 occurrences\n",
            "asparagus: 2 occurrences\n",
            "red: 2 occurrences\n",
            "soup: 2 occurrences\n",
            "chicken: 2 occurrences\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[43, 37, 0.8604651162790697, 259]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Recipe_combined_tokens = [token for sublist in df['Recipe_tokens'] for token in sublist]\n",
        "#descriptive_stats(sza_combined_tokens)\n",
        "#tokens_without_stopwords = remove_stop(Recipe_combined_tokens)\n",
        "\n",
        "#tokens_with_punctuation = remove_punctuation(tokens_without_stopwords)\n",
        "tokens_without_stopwords = Recipe_combined_tokens\n",
        "descriptive_stats(tokens_without_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM-QqRQgdOdn",
        "outputId": "a0d00e0a-d44c-4170-f050-6608156420ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cannellini',\n",
              " 'bean',\n",
              " 'asparagus',\n",
              " 'salad',\n",
              " 'mushrooms',\n",
              " 'berry',\n",
              " 'banana',\n",
              " 'breakfast',\n",
              " 'smoothie',\n",
              " 'red',\n",
              " 'lentil',\n",
              " 'soup',\n",
              " 'chicken',\n",
              " 'turnips',\n",
              " 'asparagus',\n",
              " 'pea',\n",
              " 'soup',\n",
              " 'real',\n",
              " 'convenience',\n",
              " 'food',\n",
              " 'garlicky',\n",
              " 'kale',\n",
              " 'slow',\n",
              " 'cooker',\n",
              " 'beef',\n",
              " 'stew',\n",
              " 'red',\n",
              " 'kidney',\n",
              " 'bean',\n",
              " 'jambalaya',\n",
              " 'chicken',\n",
              " 'fajita',\n",
              " 'stuffed',\n",
              " 'bell',\n",
              " 'pepper',\n",
              " 'cauliflower',\n",
              " 'brown',\n",
              " 'rice',\n",
              " 'vegetable',\n",
              " 'fried',\n",
              " 'rice',\n",
              " 'hummus',\n",
              " 'zaatar']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_without_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTTFFvjAdOdn",
        "outputId": "de4fbfa7-13fb-4161-b66c-df8358e2f699"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Count Vectorizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m count_ingredient_vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m count_ingredient_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mcount_ingredient_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_without_stopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m count_ingredient_vectors\u001b[38;5;241m.\u001b[39mshape\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1401\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1400\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[1;32m-> 1401\u001b[0m X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_limit_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1253\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m   1251\u001b[0m kept_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X[:, kept_indices], removed_terms\n",
            "\u001b[1;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
          ]
        }
      ],
      "source": [
        "# Count Vectorizer\n",
        "count_ingredient_vectorizer = CountVectorizer(stop_words='english', min_df=10, max_df=0.5)\n",
        "count_ingredient_vectors = count_ingredient_vectorizer.fit_transform(tokens_without_stopwords)\n",
        "count_ingredient_vectors.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKK45FjnpK4E",
        "outputId": "a7c0f40c-4584-4b5d-a4b8-6ac8271d4e0f"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TF-IDF Vectorizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tfidf_ingredient_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m tfidf_ingredient_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_ingredient_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_without_stopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m tfidf_ingredient_vectors\u001b[38;5;241m.\u001b[39mshape\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1401\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1400\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[1;32m-> 1401\u001b[0m X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_limit_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_doc_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1405\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1253\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m   1251\u001b[0m kept_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X[:, kept_indices], removed_terms\n",
            "\u001b[1;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
          ]
        }
      ],
      "source": [
        "# TF-IDF Vectorizer\n",
        "tfidf_ingredient_vectorizer = TfidfVectorizer(stop_words='english', min_df=5, max_df=0.7)\n",
        "tfidf_ingredient_vectors = tfidf_ingredient_vectorizer.fit_transform(tokens_without_stopwords)\n",
        "tfidf_ingredient_vectors.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Topic 00\n",
            "  chicken (100.00)\n",
            "  red (0.00)\n",
            "  rice (0.00)\n",
            "  grilled (0.00)\n",
            "  cauliflower (0.00)\n",
            "\n",
            "Topic 01\n",
            "  roasted (100.00)\n",
            "  soup (0.00)\n",
            "  salad (0.00)\n",
            "  garlic (0.00)\n",
            "  lemon (0.00)\n",
            "\n",
            "Topic 02\n",
            "  beans (100.00)\n",
            "  rice (0.00)\n",
            "  cauliflower (0.00)\n",
            "  grilled (0.00)\n",
            "  cream (0.00)\n",
            "\n",
            "Topic 03\n",
            "  oil (99.98)\n",
            "  soup (0.01)\n",
            "  red (0.00)\n",
            "  garlic (0.00)\n",
            "  cauliflower (0.00)\n",
            "\n",
            "Topic 04\n",
            "  pepper (99.93)\n",
            "  red (0.03)\n",
            "  soup (0.01)\n",
            "  cauliflower (0.01)\n",
            "  rice (0.01)\n"
          ]
        }
      ],
      "source": [
        "# NFM Model\n",
        "nmf_ingredient_model = NMF(n_components=5, random_state=314)\n",
        "W_ingredient_matrix = nmf_ingredient_model.fit_transform(tfidf_ingredient_vectors)\n",
        "H_ingredient_matrix = nmf_ingredient_model.components_\n",
        "\n",
        "# Display NMF Model\n",
        "display_topics(nmf_ingredient_model, tfidf_ingredient_vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VMmCQhXo9xg",
        "outputId": "9ff68480-3bf7-4c79-96d1-d30facd688ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Topic 00\n",
            "  beans (10.90)\n",
            "  roasted (10.90)\n",
            "  rice (8.13)\n",
            "  cream (6.43)\n",
            "  kidney (4.30)\n",
            "\n",
            "Topic 01\n",
            "  oil (10.66)\n",
            "  salt (5.89)\n",
            "  sauce (5.68)\n",
            "  chickpeas (5.24)\n",
            "  seeds (5.03)\n",
            "\n",
            "Topic 02\n",
            "  chicken (14.02)\n",
            "  olive (7.93)\n",
            "  beef (7.42)\n",
            "  green (7.42)\n",
            "  butter (6.40)\n",
            "\n",
            "Topic 03\n",
            "  pepper (9.57)\n",
            "  red (7.71)\n",
            "  grilled (7.29)\n",
            "  lemon (6.46)\n",
            "  yogurt (5.43)\n",
            "\n",
            "Topic 04\n",
            "  soup (7.07)\n",
            "  cauliflower (6.53)\n",
            "  salad (5.81)\n",
            "  garlic (5.81)\n",
            "  vegetable (4.36)\n"
          ]
        }
      ],
      "source": [
        "# Fitting LDA Model\n",
        "lda_ingredient_model = LatentDirichletAllocation(n_components=5, random_state=314)\n",
        "W_lda_ingredient_matrix = lda_ingredient_model.fit_transform(count_ingredient_vectors)\n",
        "H_lda_ingredient_matrix = lda_ingredient_model.components_\n",
        "\n",
        "# Display LDA Model\n",
        "display_topics(lda_ingredient_model, count_ingredient_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ2_mbJnlwnH",
        "outputId": "c8f785db-8647-43ca-849d-d08555418bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Topic 00\n",
            "  chicken (101.68)\n",
            "  olive (1.08)\n",
            "  oil (0.95)\n",
            "  red (0.78)\n",
            "  green (0.60)\n",
            "\n",
            "Topic 01\n",
            "  roasted (91.75)\n",
            "  beans (7.97)\n",
            "  oil (1.42)\n",
            "  olive (0.54)\n",
            "  beef (0.44)\n",
            "\n",
            "Topic 02\n",
            "  beans (111.25)\n",
            "  cream (1.94)\n",
            "  lemon (0.84)\n",
            "  garlic (0.76)\n",
            "  sauce (0.53)\n",
            "\n",
            "Topic 03\n",
            "  oil (98.72)\n",
            "  cream (5.60)\n",
            "  garlic (3.62)\n",
            "  pepper (1.75)\n",
            "  beans (1.55)\n",
            "\n",
            "Topic 04\n",
            "  pepper (101.26)\n",
            "  red (3.35)\n",
            "  green (3.22)\n",
            "  olive (2.24)\n",
            "  cauliflower (1.85)\n"
          ]
        }
      ],
      "source": [
        "# Fitting LSA Model\n",
        "svd_ingredient_model = TruncatedSVD(n_components=5, random_state=314)\n",
        "W_svd_ingredient_matrix = svd_ingredient_model.fit_transform(tfidf_ingredient_vectors)\n",
        "H_svd_ingredient_matrix = svd_ingredient_model.components_\n",
        "# Display LSA Model\n",
        "display_topics(svd_ingredient_model, tfidf_ingredient_vectorizer.get_feature_names_out())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ADS500B",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
